{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization:\n",
    "# Devices on which models are to be transferred, Mobile device for example have constraints like limited\n",
    "# memory, processing power, etc. So, the model's weights and biases values must be quantized. For example, \n",
    "# from 32-bit sized integer to 8-bit sized integer.\n",
    "# It can be done during training(Quantization Aware Training) or after training(Post Training Quantization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantizing only weights using OPTIMIZE_FOR_SIZE option and Assuming model is saved in SavedModel format\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"tmp/saved_model\")\n",
    "\n",
    "# A list of optimizations to do on model\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantizing activation outputs using DEFAULT option. For this, tfLite needs to know in advanced dynamic range\n",
    "# of activation output to optimize it properly so that it doesn't deteriorate the model. So, we create a \n",
    "# data generator for tfLite to get this range\n",
    "def generator():\n",
    "    data = tfds.load(...)\n",
    "    for _ in range(num_calibration_steps):\n",
    "        image, = data.take(1)\n",
    "        yield [image]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Passing generator to generate Representative data from which activation outputs will be optimized\n",
    "converter.representative_dataset = tf.lite.RepresentativeDataset(generator)\n",
    "\n",
    "# Restricting supported target quantization operation to INT8\n",
    "converter.target_specs.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
